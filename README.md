# Detecting AI-Generated Scientific Papers
<img width="944" height="90" alt="image" src="https://github.com/user-attachments/assets/30ea8b53-f9f5-4a47-9008-1fd5b9c16bee" />

This repository contains a solution for the Kaggle competition [Detecting Generated Scientific Papers](https://www.kaggle.com/competitions/detecting-generated-scientific-papers/overview), where the task is to classify whether a scientific abstract was written by a human or generated by AI.


## Overview

- **Task**: Binary text classification (AI-generated vs. human-written scientific papers).  
- **Input**: Abstracts of scientific papers in plain text format.  
- **Datasets**:
  - **Train**: ~21k samples (15k AI-generated, 7k human-written).  
  - **Test**: ~5k samples for evaluation.  
- **Evaluation Metric**: F1-score.  

Dataset preprocessing shows average text length ~140 words, with values ranging from 50 to 1500. The dataset is imbalanced, with more AI-generated samples than human-written.


## Solution

- **Model**: Fine-tuned **BERT (bert-base-uncased)** for binary classification.  
- **Training setup**:
  - Input text tokenised with `BertTokenizer` (max length = 512).  
  - Backbone: pre-trained BERT model.  
  - Classifier head: Fully connected layers with dropout and ReLU activation.  
  - Optimiser: AdamW with different learning rates for backbone and classifier.  
  - Loss: Binary Cross-Entropy with Logits.  
  - Mixed precision training with gradient scaling.  
- **Validation**: 80/20 stratified split of train data.  
- **Metrics**: F1-score, precision, recall, confusion matrix.  
- **Results**:  
  - Validation F1: ~0.99 after 5 epochs (1 hour of T4 GPU runtime).
  - Test F1: ~0.99 (excellent precision and recall).  

### Repository structure
- **solution.ipynb** – Jupyter Notebook with full pipeline: data loading, preprocessing, model training, evaluation.  
- **[best_bert_binary.pt](https://mega.nz/file/ujZgkBBZ#wKmEORQ2hekdeIAYM2eXL6yEF88zzxt8uNnY9tUYly0)** – Saved weights of the best fine-tuned BERT model.  
